spring.application.name=resume-ai-backend
#spring.ai.ollama.chat.model=deepseek-r1:7b
spring.ai.ollama.chat.model=tinyllama:latest

# Ollama configuration
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.temperature=0.7
spring.ai.ollama.chat.options.num-predict=512

# Performance optimizations
spring.ai.ollama.chat.options.num-ctx=1024
spring.ai.ollama.chat.options.repeat-penalty=1.1
spring.ai.ollama.chat.options.top-k=40
spring.ai.ollama.chat.options.top-p=0.9

# Disable streaming for more reliable responses
spring.ai.ollama.chat.options.stream=false

server.port=8080
