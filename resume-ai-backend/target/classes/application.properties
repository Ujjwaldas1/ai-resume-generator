spring.application.name=resume-ai-backend

# Ollama Configuration
# Make sure Ollama is running: ollama serve
# Install model: ollama pull deepseek-r1:1.5b
# Or use another model like: llama3.2, mistral, etc.
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=deepseek-r1:1.5b

# More stable responses
spring.ai.ollama.chat.options.temperature=0.2
spring.ai.ollama.chat.options.stream=false

server.port=8080

spring.jackson.serialization.fail-on-empty-beans=false

# Logging for debugging
logging.level.com.resume.backend=DEBUG
logging.level.org.springframework.ai=DEBUG
